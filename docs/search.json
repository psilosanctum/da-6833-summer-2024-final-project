[
  {
    "objectID": "data/processed-data/readme.html",
    "href": "data/processed-data/readme.html",
    "title": "processed-data",
    "section": "",
    "text": "processed-data\nThis folder contains data that has been processed and cleaned by code.\nAny files located in here are based on the raw data and can be re-created running the various processing/cleaning code scripts in the code folder.\nYou could add a codebook here, but you could also just provide enough comments in the code that produces the content in this folder for users to understand what is saved in this location."
  },
  {
    "objectID": "code/processing-code/readme.html",
    "href": "code/processing-code/readme.html",
    "title": "processing-code",
    "section": "",
    "text": "processing-code\nThis folder contains code for processing data.\nIt currently contains 3 example files, showing the same processing steps done using slightly different setup with R and Quarto.\n\nFirst, there is an R script that you can run which does all the cleaning.\nSecond, there is a Quarto file which contains exactly the same code as the R script, with some comments. Everything lives inside the Quarto file.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. You can also mix and match. For instance for an EDA task, it might make sense to produce a Quarto file. Then I would use the 2nd or 3rd approach. If you do a main analysis, then you might just want to have an R script that does the data analysis and saves the results to a file, for later use/processing. You might not need or want a quarto file for that.\nWhichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "code/eda-code/eda-ev-dataset.html",
    "href": "code/eda-code/eda-ev-dataset.html",
    "title": "EV Exploratory Analysis",
    "section": "",
    "text": "library(readxl) #for loading Excel files\nlibrary(readr)\nlibrary(dplyr) #for data processing/cleaning\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(tidycensus)\n\n\nIntroduction\nThis document is an exploratory data analysis of electric vehicle data. The data was obtained from the IRS and contains information on electric vehicles, their owners, and the tax benefits they receive. The data was cleaned and processed in R and saved as a CSV file. This document will load the data, explore it, and fit a linear regression model to predict the number of electric vehicles in a given zip code.\n\n# Import Data, Check Descriptive Statistics & Data Types\ndata_location &lt;- here::here(\"data\",\"raw-data\",\"Electric_Vehicle_Population_Data.csv\")\nev_data &lt;- read.csv(data_location)\nsummary(ev_data)\n\n  VIN..1.10.           County              City              State          \n Length:181458      Length:181458      Length:181458      Length:181458     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  Postal.Code      Model.Year       Make              Model          \n Min.   : 1545   Min.   :1997   Length:181458      Length:181458     \n 1st Qu.:98052   1st Qu.:2019   Class :character   Class :character  \n Median :98122   Median :2022   Mode  :character   Mode  :character  \n Mean   :98174   Mean   :2021                                        \n 3rd Qu.:98370   3rd Qu.:2023                                        \n Max.   :99577   Max.   :2024                                        \n NA's   :3                                                           \n Electric.Vehicle.Type Clean.Alternative.Fuel.Vehicle..CAFV..Eligibility\n Length:181458         Length:181458                                    \n Class :character      Class :character                                 \n Mode  :character      Mode  :character                                 \n                                                                        \n                                                                        \n                                                                        \n                                                                        \n Electric.Range     Base.MSRP      Legislative.District DOL.Vehicle.ID     \n Min.   :  0.00   Min.   :     0   Min.   : 1.00        Min.   :     4385  \n 1st Qu.:  0.00   1st Qu.:     0   1st Qu.:18.00        1st Qu.:183068667  \n Median :  0.00   Median :     0   Median :33.00        Median :228915522  \n Mean   : 57.83   Mean   :  1040   Mean   :29.11        Mean   :221412778  \n 3rd Qu.: 75.00   3rd Qu.:     0   3rd Qu.:42.00        3rd Qu.:256131982  \n Max.   :337.00   Max.   :845000   Max.   :49.00        Max.   :479254772  \n                                   NA's   :398                             \n Vehicle.Location   Electric.Utility   X2020.Census.Tract \n Length:181458      Length:181458      Min.   :1.001e+09  \n Class :character   Class :character   1st Qu.:5.303e+10  \n Mode  :character   Mode  :character   Median :5.303e+10  \n                                       Mean   :5.298e+10  \n                                       3rd Qu.:5.305e+10  \n                                       Max.   :5.603e+10  \n                                       NA's   :3          \n\ndplyr::glimpse(ev_data)\n\nRows: 181,458\nColumns: 17\n$ VIN..1.10.                                        &lt;chr&gt; \"WAUTPBFF4H\", \"WAUUP…\n$ County                                            &lt;chr&gt; \"King\", \"Thurston\", …\n$ City                                              &lt;chr&gt; \"Seattle\", \"Olympia\"…\n$ State                                             &lt;chr&gt; \"WA\", \"WA\", \"WA\", \"W…\n$ Postal.Code                                       &lt;int&gt; 98126, 98502, 98516,…\n$ Model.Year                                        &lt;int&gt; 2017, 2018, 2017, 20…\n$ Make                                              &lt;chr&gt; \"AUDI\", \"AUDI\", \"TES…\n$ Model                                             &lt;chr&gt; \"A3\", \"A3\", \"MODEL S…\n$ Electric.Vehicle.Type                             &lt;chr&gt; \"Plug-in Hybrid Elec…\n$ Clean.Alternative.Fuel.Vehicle..CAFV..Eligibility &lt;chr&gt; \"Not eligible due to…\n$ Electric.Range                                    &lt;int&gt; 16, 16, 210, 25, 308…\n$ Base.MSRP                                         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0,…\n$ Legislative.District                              &lt;int&gt; 34, 22, 22, 20, 14, …\n$ DOL.Vehicle.ID                                    &lt;int&gt; 235085336, 237896795…\n$ Vehicle.Location                                  &lt;chr&gt; \"POINT (-122.374105 …\n$ Electric.Utility                                  &lt;chr&gt; \"CITY OF SEATTLE - (…\n$ X2020.Census.Tract                                &lt;dbl&gt; 53033011500, 5306701…\n\n\n\n# Checking for duplicates:\nduplicates &lt;- duplicated(ev_data) # Check for duplicates\nnum_duplicates &lt;- sum(duplicates) # Count of duplicates\nprint(num_duplicates)\n\n[1] 0\n\n\n\n# Find total rows with MSRP prices\ncount_zero_msrp = length(which(ev_data$Base.MSRP == 0))\ntotal_rows_count = nrow(ev_data)\ntotal_price_data_points = total_rows_count - count_zero_msrp\nprint(paste0('Total rows: ', total_rows_count))\n\n[1] \"Total rows: 181458\"\n\nprint(paste0('Total Missing MSRP Prices: ', count_zero_msrp))\n\n[1] \"Total Missing MSRP Prices: 178146\"\n\nprint(paste0('Total Price Data Points: ', total_price_data_points))\n\n[1] \"Total Price Data Points: 3312\"\n\n\n\n# Checking for Null values:\ntotal_na &lt;- sum(is.na(ev_data))\nprint(total_na)\n\n[1] 404\n\n\n\n# Removing null values and outlier from MSRP:\nev_data_filtered &lt;- na.omit(ev_data) # FIltered data with NA values removed\nmax_msrp_outlier &lt;- ev_data_filtered %&gt;%\n  group_by(Electric.Vehicle.Type) %&gt;%\n  summarise(max = max(Base.MSRP)) \nmax_msrp_outlier\n\n# A tibble: 2 × 2\n  Electric.Vehicle.Type                     max\n  &lt;chr&gt;                                   &lt;int&gt;\n1 Battery Electric Vehicle (BEV)         110950\n2 Plug-in Hybrid Electric Vehicle (PHEV) 845000\n\nfilter_msrp_outlier &lt;- ev_data_filtered %&gt;%\n  filter(Base.MSRP != 845000) %&gt;%\n  filter(Base.MSRP != 0)\n\n\n# Create a boxplot of price distribution by electric vehicle type\np2_boxplot&lt;-ggplot(filter_msrp_outlier, aes(x = Electric.Vehicle.Type, y = Base.MSRP)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot - Price Distribution vs. Electric Vehicle Type\",\n       x = \"Electric Vehicle Type\",\n       y = \"MSRP\")\np2_boxplot \n\n\n\n\n\n# Clean data for plot\nprice_by_state = ev_data_filtered %&gt;% \n  group_by(Clean.Alternative.Fuel.Vehicle..CAFV..Eligibility) %&gt;%\n  filter(Base.MSRP != 0) %&gt;%\n  filter(Base.MSRP != 845000)\n\n# Create a boxplot for price distribution by alternative fuel eligibility\np3_boxplot&lt;-ggplot(price_by_state, aes(x = Clean.Alternative.Fuel.Vehicle..CAFV..Eligibility, y = Base.MSRP)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot - Price Distribution vs. Alt. Vehicle Eligibility\",\n       x = \"Electric Vehicle Type\",\n       y = \"MSRP\")\np3_boxplot \n\n\n\n\n\n# Clean data from plot\nprice_by_state = ev_data_filtered %&gt;% \n  group_by(Clean.Alternative.Fuel.Vehicle..CAFV..Eligibility) %&gt;%\n  filter(Electric.Range != 0) \n\n# Create a boxplot\np4_boxplot&lt;-ggplot(price_by_state, aes(x = Clean.Alternative.Fuel.Vehicle..CAFV..Eligibility, y = Electric.Range)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot - Price Distribution vs. Electric Vehicle Type\")\np4_boxplot \n\n\n\n\n\n# Create a boxplot with the new categorical variable on the x-axis and height on the y-axis\np_boxplot&lt;-ggplot(filter_msrp_outlier, aes(x = Make, y = Base.MSRP)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot - Price Distribution by Brand\",\n       x = \"Brand\",\n       y = \"Price\")\np_boxplot\n\n\n\n\n\n# Create a scatterplot with electric vehicle range growth over time\n\np_scatterplot&lt;-ggplot(ev_data_filtered, aes(x = Model.Year, y = Electric.Range)) +\n  geom_point() +\n  labs(title = \"Scatterplot - Electric Vehicle Range Growth over Time\",\n       x = \"Year\",\n       y = \"Electric Range\")\nplot(p_scatterplot)\n\n\n\n# Save the scatterplot to a file\n# ggsave(\"scatterplot.png\", plot = p_scatterplot)"
  },
  {
    "objectID": "code/eda-code/readme.html",
    "href": "code/eda-code/readme.html",
    "title": "eda-code",
    "section": "",
    "text": "eda-code\nThis folder contains code to do a simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder.\nIt’s the same code done 3 times. For explanations on the 3 different ways, see the readme file in the processing-code folder."
  },
  {
    "objectID": "code/analysis-code/statistical_analysis.html",
    "href": "code/analysis-code/statistical_analysis.html",
    "title": "Statistical Analysis",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport re\npd.set_option('display.max_rows', None)\n\n# Load the dataset\ndata = pd.read_csv('../../data/processed-data/brand_model_year_tax_data.csv').drop(\n    [\n        'The State Federal Information Processing System (FIPS) code',\n        'The State associated with the ZIP code',\n        'Number of returns [3]',\n        'City',\n        'State',\n        'Model Year',\n        'Make',\n        'Model',\n        'Electric Vehicle Type',\n        'Clean Alternative Fuel Vehicle (CAFV) Eligibility',\n    ],\n    axis = 1\n)\n\n# Remove non-letters from columns names\nregex = re.compile(r\"\\[|\\]|&lt;\", re.IGNORECASE)\n\ndata.columns = [regex.sub(\"\", col) if any(x in str(col) for x in set(('[', ']', '&lt;'))) else col for col in data.columns.values]\n\ndata.columns = [re.sub(r'[0-9]', '', col).strip() for col in data.columns]\ndata.columns = [re.sub(r'/', '_', col) for col in data.columns]\n\n# Group by 'ZIPCODE' and sum the 'Vehicle Count'\ngrouped_data = data.groupby('ZIPCODE')['Vehicle Count'].sum().reset_index()\n\n# Selecting columns L to CY (indexes 11 to 102)\nfeatures_to_use = data.columns[3:-1]\n\n# Ensuring correct selection of numeric columns\nnumeric_features_to_use = data[features_to_use].select_dtypes(include=['number']).columns\n\n# Grouping these features by ZIPCODE and summing them\ngrouped_features = data.groupby('ZIPCODE')[numeric_features_to_use].sum().reset_index()\n\n# Merging the summed Vehicle Count with the grouped features on ZIPCODE\nmerged_data = grouped_data.merge(grouped_features, on='ZIPCODE')\n\n# Extracting the features (X) and target (y)\nX_grouped = merged_data[numeric_features_to_use]\ny_grouped = merged_data['Vehicle Count']\n\n# Scale data\nscalar = StandardScaler()\nscale = scalar.fit_transform(X_grouped)\n\n# Splitting the data into training and testing sets\nX_train_grouped, X_test_grouped, y_train_grouped, y_test_grouped = train_test_split(\n    X_grouped, y_grouped, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "code/analysis-code/statistical_analysis.html#import-libraries-data",
    "href": "code/analysis-code/statistical_analysis.html#import-libraries-data",
    "title": "Statistical Analysis",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport re\npd.set_option('display.max_rows', None)\n\n# Load the dataset\ndata = pd.read_csv('../../data/processed-data/brand_model_year_tax_data.csv').drop(\n    [\n        'The State Federal Information Processing System (FIPS) code',\n        'The State associated with the ZIP code',\n        'Number of returns [3]',\n        'City',\n        'State',\n        'Model Year',\n        'Make',\n        'Model',\n        'Electric Vehicle Type',\n        'Clean Alternative Fuel Vehicle (CAFV) Eligibility',\n    ],\n    axis = 1\n)\n\n# Remove non-letters from columns names\nregex = re.compile(r\"\\[|\\]|&lt;\", re.IGNORECASE)\n\ndata.columns = [regex.sub(\"\", col) if any(x in str(col) for x in set(('[', ']', '&lt;'))) else col for col in data.columns.values]\n\ndata.columns = [re.sub(r'[0-9]', '', col).strip() for col in data.columns]\ndata.columns = [re.sub(r'/', '_', col) for col in data.columns]\n\n# Group by 'ZIPCODE' and sum the 'Vehicle Count'\ngrouped_data = data.groupby('ZIPCODE')['Vehicle Count'].sum().reset_index()\n\n# Selecting columns L to CY (indexes 11 to 102)\nfeatures_to_use = data.columns[3:-1]\n\n# Ensuring correct selection of numeric columns\nnumeric_features_to_use = data[features_to_use].select_dtypes(include=['number']).columns\n\n# Grouping these features by ZIPCODE and summing them\ngrouped_features = data.groupby('ZIPCODE')[numeric_features_to_use].sum().reset_index()\n\n# Merging the summed Vehicle Count with the grouped features on ZIPCODE\nmerged_data = grouped_data.merge(grouped_features, on='ZIPCODE')\n\n# Extracting the features (X) and target (y)\nX_grouped = merged_data[numeric_features_to_use]\ny_grouped = merged_data['Vehicle Count']\n\n# Scale data\nscalar = StandardScaler()\nscale = scalar.fit_transform(X_grouped)\n\n# Splitting the data into training and testing sets\nX_train_grouped, X_test_grouped, y_train_grouped, y_test_grouped = train_test_split(\n    X_grouped, y_grouped, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "code/analysis-code/statistical_analysis.html#preprocess-transform-and-build-models",
    "href": "code/analysis-code/statistical_analysis.html#preprocess-transform-and-build-models",
    "title": "Statistical Analysis",
    "section": "Preprocess, Transform, and Build Models",
    "text": "Preprocess, Transform, and Build Models\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import StackingRegressor, HistGradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport scipy.stats as stats\n\n# Define a function to log transform all features\ndef log_transform_all_features(data):\n    return np.log1p(data)\n\n# Impute missing values with the median and then log transform all features\ndef preprocess_and_log_transform_data(X):\n    imputer = SimpleImputer(strategy='median')\n    X_imputed = imputer.fit_transform(X)\n    X_imputed_df = pd.DataFrame(X_imputed, columns=X.columns)\n    X_transformed = log_transform_all_features(X_imputed_df)\n    return X_transformed\n\n# Preprocess and log transform the training and test datasets\nX_train_transformed = preprocess_and_log_transform_data(X_train_grouped)\nX_test_transformed = preprocess_and_log_transform_data(X_test_grouped)\n\n\n# Define the models\nmodels = {\n    'HistGradientBoosting_Regressor': HistGradientBoostingRegressor(),\n    'Decision_Tree_Regressor': DecisionTreeRegressor(max_depth=5, random_state=42),\n    'XGBoost_Regressor': xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42),\n    'Random_Forest_Regressor': RandomForestRegressor(random_state=42),\n}\n\n# Function to plot Q-Q plot, residuals distribution, and predicted vs. actual\ndef plot_model_diagnostics(y_true, y_pred, model_name):\n    residuals = y_true - y_pred\n    \n    plt.figure(figsize=(18, 10))\n    \n    # Residuals Distribution\n    plt.subplot(1, 2, 1)\n    sns.histplot(residuals, kde=True, bins=30)\n    plt.title(f'Residuals Distribution for {model_name}')\n    plt.xlabel('Residuals')\n    plt.ylabel('Frequency')\n    \n    # Predicted vs Actual\n    plt.subplot(1, 2, 2)\n    plt.scatter(y_true, y_pred, alpha=0.5, edgecolor='k')\n    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n    plt.title(f'Predicted vs. Actual for {model_name}')\n    plt.xlabel('Actual Values')\n    plt.ylabel('Predicted Values')\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig(f'../../results/output/model_results/{model_name}.png')\n    plt.show()\n\n# Train each model and plot diagnostics\nfor name, model in models.items():\n    model.fit(X_train_transformed, y_train_grouped)\n    predictions = model.predict(X_test_transformed)\n    \n    # Print model evaluation metrics\n    rmse = mean_squared_error(y_test_grouped, predictions, squared=False)\n    r2 = r2_score(y_test_grouped, predictions)\n    print(f\"{name} - RMSE: {rmse}, R-squared: {r2}\")\n    \n    # Plot model diagnostics\n    plot_model_diagnostics(y_test_grouped, predictions, name)\n\nHistGradientBoosting_Regressor - RMSE: 76.95904869760996, R-squared: 0.8665557984630057\nDecision_Tree_Regressor - RMSE: 79.61198784609248, R-squared: 0.8571970227079834\nXGBoost_Regressor - RMSE: 69.21407326176023, R-squared: 0.8920632600784302\nRandom_Forest_Regressor - RMSE: 71.30168705871802, R-squared: 0.8854539974855006"
  },
  {
    "objectID": "code/analysis-code/statistical_analysis.html#variable-importance---top-10",
    "href": "code/analysis-code/statistical_analysis.html#variable-importance---top-10",
    "title": "Statistical Analysis",
    "section": "Variable Importance - Top 10",
    "text": "Variable Importance - Top 10\n\nfrom sklearn.inspection import permutation_importance\nimport pandas as pd\n\n# Train the models on the transformed training data\nxgboost_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\nhist_gb_model = HistGradientBoostingRegressor()\ndec_tree_model = DecisionTreeRegressor(max_depth=5, random_state=42)\nrand_for_model = RandomForestRegressor(random_state=42)\n\nxgboost_model.fit(X_train_transformed, y_train_grouped)\nhist_gb_model.fit(X_train_transformed, y_train_grouped)\ndec_tree_model.fit(X_train_transformed, y_train_grouped)\nrand_for_model.fit(X_train_transformed, y_train_grouped)\n\n# Get feature names from the transformed data\nfeature_names = X_train_transformed.columns\n\n# Extract feature importances from XGBoost\ndec_tree_importances = dec_tree_model.feature_importances_\ndec_tree_feature_importance = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': dec_tree_importances\n}).sort_values(by='Importance', ascending=False)\n\n# Extract feature importances from XGBoost\nxgboost_importances = xgboost_model.feature_importances_\nxgboost_feature_importance = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': xgboost_importances\n}).sort_values(by='Importance', ascending=False)\n\n# Compute feature importances using permutation importance for HistGradientBoostingRegressor\nhist_gb_importances = permutation_importance(hist_gb_model, X_test_transformed, y_test_grouped, n_repeats=10, random_state=42)\nhist_gb_feature_importance = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': hist_gb_importances.importances_mean\n}).sort_values(by='Importance', ascending=False)\n\n# Extract feature importances from XGBoost\nrand_for_importances = rand_for_model.feature_importances_\nrand_for_feature_importance = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': rand_for_importances\n}).sort_values(by='Importance', ascending=False)\n\n# Display the top 10 important features from each model\nprint(\"Top 10 Important Features from XGBoost:\")\nprint(xgboost_feature_importance.head(10))\n\nprint(\"\\nTop 10 Important Features from HistGradientBoostingRegressor (Permutation Importance):\")\nprint(hist_gb_feature_importance.head(10))\n\nprint(\"\\nTop 10 Important Features from Decision Tree:\")\nprint(dec_tree_feature_importance.head(10))\n\nprint(\"\\nTop 10 Important Features from Random Forest:\")\nprint(rand_for_feature_importance.head(10))\n\n# Plotting function\ndef plot_feature_importance(importances, model_name):\n    plt.figure(figsize=(10, 5))\n    plt.barh(importances['Feature'], importances['Importance'], color='skyblue')\n    plt.xlabel('Importance')\n    plt.ylabel('Feature')\n    plt.title(f'Feature Importance - {model_name}')\n    plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(f'../../results/output/variable_importance/{model_name}.png')\n    plt.show()\n\n\n# Plotting feature importance for XGBoost\nplot_feature_importance(xgboost_feature_importance.head(10), 'XGBoost')\n\n# Plotting feature importance for HistGradientBoostingRegressor (Permutation Importance)\nplot_feature_importance(hist_gb_feature_importance.head(10), 'HistGradientBoostingRegressor (Permutation Importance)')\n\n# Plotting feature importance for HistGradientBoostingRegressor (Permutation Importance)\nplot_feature_importance(rand_for_feature_importance.head(10), 'Random Forest Regressor')\n\n# Plotting feature importance for HistGradientBoostingRegressor (Permutation Importance)\nplot_feature_importance(dec_tree_feature_importance.head(10), 'Decision Tree Regressor')\n\nTop 10 Important Features from XGBoost:\n                                              Feature  Importance\n29    Self-employed health insurance deduction amount    0.633054\n30  Individual retirement arrangement payments amount    0.073026\n42          Home mortgage from personal seller amount    0.048981\n14                          Salaries and wages amount    0.043235\n41                 Home mortgage interest paid amount    0.036040\n56                         Self-employment tax amount    0.032866\n28      Self-employed (Keogh) retirement plans amount    0.030317\n19  Business or professional net income (less loss...    0.021596\n38                           Real estate taxes amount    0.017873\n62                      Net premium tax credit amount    0.008516\n\nTop 10 Important Features from HistGradientBoostingRegressor (Permutation Importance):\n                                              Feature  Importance\n30  Individual retirement arrangement payments amount    0.036520\n29    Self-employed health insurance deduction amount    0.022240\n28      Self-employed (Keogh) retirement plans amount    0.014973\n62                      Net premium tax credit amount    0.011162\n46              Total charitable contributions amount    0.010919\n42          Home mortgage from personal seller amount    0.009908\n50                          Foreign tax credit amount    0.007443\n38                           Real estate taxes amount    0.005748\n41                 Home mortgage interest paid amount    0.005061\n48                Qualified business income deduction    0.005041\n\nTop 10 Important Features from Decision Tree:\n                                              Feature  Importance\n29    Self-employed health insurance deduction amount    0.655552\n41                 Home mortgage interest paid amount    0.145029\n30  Individual retirement arrangement payments amount    0.096401\n14                          Salaries and wages amount    0.041032\n7   Number of returns with virtual currency indicator    0.024793\n38                           Real estate taxes amount    0.015771\n31             Student loan interest deduction amount    0.007102\n40                      Limited state and local taxes    0.002781\n20                Net capital gain (less loss) amount    0.002534\n69                   Net investment income tax amount    0.001792\n\nTop 10 Important Features from Random Forest:\n                                              Feature  Importance\n29    Self-employed health insurance deduction amount    0.190544\n62                      Net premium tax credit amount    0.173105\n14                          Salaries and wages amount    0.097192\n38                           Real estate taxes amount    0.076039\n20                Net capital gain (less loss) amount    0.069616\n42          Home mortgage from personal seller amount    0.056789\n28      Self-employed (Keogh) retirement plans amount    0.048481\n30  Individual retirement arrangement payments amount    0.042663\n41                 Home mortgage interest paid amount    0.038645\n40                      Limited state and local taxes    0.033366"
  },
  {
    "objectID": "code/analysis-code/statistical_analysis.html#compare-models",
    "href": "code/analysis-code/statistical_analysis.html#compare-models",
    "title": "Statistical Analysis",
    "section": "Compare Models",
    "text": "Compare Models\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport pandas as pd\n\n# Function to evaluate models and compare various metrics\ndef compare_model_metrics(models, X_train, y_train, X_test, y_test):\n    results = []\n\n    for name, model in models.items():\n        # Train the model\n        model.fit(X_train, y_train)\n        # Predict on the test set\n        predictions = model.predict(X_test)\n        # Calculate metrics\n        rmse = mean_squared_error(y_test, predictions, squared=False)\n        r2 = r2_score(y_test, predictions)\n        mae = mean_absolute_error(y_test, predictions)\n        mse = mean_squared_error(y_test, predictions)\n        # Store the results\n        results.append({\n            'Model': name,\n            'RMSE': rmse,\n            'R-squared': r2,\n            'MAE': mae,\n            'MSE': mse\n        })\n    \n    # Convert the results to a DataFrame for easy comparison\n    return pd.DataFrame(results)\n\n# Models to Compare\nmodels_to_compare = {\n    'XGBoost_Regressor': xgboost_model,  # Assuming xgboost_model is already defined\n    'Decision_Tree_Regressor': DecisionTreeRegressor(max_depth=5, random_state=42),\n    'HistGradientBoosting_Regressor': hist_gb_model,  # Assuming hist_gb_model is already defined\n    'Random_Forest_Regressor': RandomForestRegressor(n_estimators=100, random_state=42),\n}\n\n# Evaluate and compare the models\nmodel_comparison_metrics_df = compare_model_metrics(models_to_compare, X_train_transformed, y_train_grouped, X_test_transformed, y_test_grouped)\n\n# Display the comparison results\nmodel_comparison_metrics_df.to_csv('../../results/output/model_comparison.csv', index=False)\n\n\nModel Comparison Results\n\nprint(model_comparison_metrics_df)\n\n                            Model       RMSE  R-squared        MAE  \\\n0               XGBoost_Regressor  69.214073   0.892063  32.473010   \n1         Decision_Tree_Regressor  79.611988   0.857197  37.917499   \n2  HistGradientBoosting_Regressor  76.959049   0.866556  36.438862   \n3         Random_Forest_Regressor  71.301687   0.885454  32.290430   \n\n           MSE  \n0  4790.587937  \n1  6338.068609  \n2  5922.695176  \n3  5083.930577"
  },
  {
    "objectID": "code/analysis-code/statistical_analysis.html#visualize-model-comparison",
    "href": "code/analysis-code/statistical_analysis.html#visualize-model-comparison",
    "title": "Statistical Analysis",
    "section": "Visualize Model Comparison",
    "text": "Visualize Model Comparison\n\nimport matplotlib.pyplot as plt\n\n# Function to visualize the model performance differences\ndef plot_model_performance_comparison(df, metric):\n    plt.figure(figsize=(10, 6))\n    df_sorted = df.sort_values(by=metric, ascending=False)\n    plt.barh(df_sorted['Model'], df_sorted[metric], color='skyblue')\n    plt.xlabel(metric)\n    plt.ylabel('Model')\n    plt.title(f'Model Comparison by {metric}')\n    plt.gca().invert_yaxis()  # Invert y-axis to have the best performing model on top\n    plt.grid(True)\n    plt.savefig(f'../../results/output/model_comparison/{metric}.png')\n    plt.show()\n\n# Visualize RMSE comparison\nplot_model_performance_comparison(model_comparison_metrics_df, 'RMSE')\n\n# Visualize R-squared comparison\nplot_model_performance_comparison(model_comparison_metrics_df, 'R-squared')\n\n# Visualize MAE comparison\nplot_model_performance_comparison(model_comparison_metrics_df, 'MAE')\n\n# Visualize MSE comparison\nplot_model_performance_comparison(model_comparison_metrics_df, 'MSE')"
  },
  {
    "objectID": "code/analysis-code/statistical_analysis.html#create-coefficients-dataframe",
    "href": "code/analysis-code/statistical_analysis.html#create-coefficients-dataframe",
    "title": "Statistical Analysis",
    "section": "Create Coefficients DataFrame",
    "text": "Create Coefficients DataFrame\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\n\n# Function to extract model coefficients and intercepts\ndef get_model_coefficients_and_intercepts(model, model_name, feature_names):\n    if hasattr(model, 'coef_'):\n        # Linear models and similar that have a coef_ attribute\n        coefficients = model.coef_\n    elif hasattr(model, 'feature_importances_'):\n        # Tree-based models with feature_importances_ attribute\n        coefficients = model.feature_importances_\n    else:\n        coefficients = np.nan  # Placeholder for models without accessible coefficients\n\n    coefficients_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Coefficient': coefficients,\n        'Model': model_name\n    })\n    \n    \n    return pd.concat([coefficients_df], ignore_index=True)\n\n# Get feature names\nfeature_names = X_train_transformed.columns\n\n# Initialize models (assuming they are already trained)\nmodels = {\n    'XGBoost_Regressor': xgboost_model,\n    'Decision_Tree_Regressor': dec_tree_model,\n    'HistGradientBoosting_Regressor': hist_gb_model,\n    'Random_Forest_Regressor': rand_for_model,\n}\n\n# Collect coefficients for each model\ncoefficients_with_intercepts_list = []\n\nfor model_name, model in models.items():\n    coefficients_with_intercepts_list.append(get_model_coefficients_and_intercepts(model, model_name, feature_names))\n\n# Combine all coefficients into one DataFrame\ncoefficients_df = pd.concat(coefficients_with_intercepts_list, ignore_index=True)\n\n# Display the coefficients DataFrame\ncoefficients_df = coefficients_df.dropna()\ncoefficients_df.to_csv('../../results/output/model_coefficients.csv', index=False)"
  },
  {
    "objectID": "code/eda-code/eda-ev-tax.html",
    "href": "code/eda-code/eda-ev-tax.html",
    "title": "Log-Transform Predictors",
    "section": "",
    "text": "Electric Vehicle Data Analysis"
  },
  {
    "objectID": "code/eda-code/eda-ev-tax.html#introduction",
    "href": "code/eda-code/eda-ev-tax.html#introduction",
    "title": "Log-Transform Predictors",
    "section": "Introduction",
    "text": "Introduction\nCreate scatterplots to see the relationship between the independent variable and predictors. Perform a log transformation on features to normalize and compare original to transformed distribution."
  },
  {
    "objectID": "code/eda-code/eda-ev-tax.html#import-libraries-data-create-traintest-datasets",
    "href": "code/eda-code/eda-ev-tax.html#import-libraries-data-create-traintest-datasets",
    "title": "Log-Transform Predictors",
    "section": "Import Libraries & Data, Create Train/Test Datasets",
    "text": "Import Libraries & Data, Create Train/Test Datasets\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re\n\n# Load the dataset\ndata = pd.read_csv('../../data/processed-data/brand_model_year_tax_data.csv').drop(\n    [\n        'The State Federal Information Processing System (FIPS) code',\n        'The State associated with the ZIP code',\n        'Number of returns [3]',\n        'City',\n        'State',\n        'Model Year',\n        'Make',\n        'Model',\n        'Electric Vehicle Type',\n        'Clean Alternative Fuel Vehicle (CAFV) Eligibility',\n    ],\n    axis = 1\n)\n\n# Remove non-letters from columns names\nregex = re.compile(r\"\\[|\\]|&lt;\", re.IGNORECASE)\n\ndata.columns = [regex.sub(\"\", col) if any(x in str(col) for x in set(('[', ']', '&lt;'))) else col for col in data.columns.values]\n\ndata.columns = [re.sub(r'[0-9]', '', col).strip() for col in data.columns]\ndata.columns = [re.sub(r'/', '_', col) for col in data.columns]\n\n# Group by 'ZIPCODE' and sum the 'Vehicle Count'\ngrouped_data = data.groupby('ZIPCODE')['Vehicle Count'].sum().reset_index()\n\n# Selecting columns L to CY (indexes 11 to 102)\nfeatures_to_use = data.columns[2:]\n\n# Ensuring correct selection of numeric columns\nnumeric_features_to_use = data[features_to_use].select_dtypes(include=['number']).columns\n\n# Grouping these features by ZIPCODE and summing them\ngrouped_features = data.groupby('ZIPCODE')[numeric_features_to_use].sum().reset_index().drop('Vehicle Count', axis = 1)\n\n# Merging the summed Vehicle Count with the grouped features on ZIPCODE\nmerged_data = grouped_data.merge(grouped_features, on='ZIPCODE')\n\n# Extracting the features (X) and target (y)\nX_grouped = merged_data[numeric_features_to_use]\ny_grouped = merged_data['Vehicle Count']\n\n# Splitting the data into training and testing sets\nX_train_grouped, X_test_grouped, y_train_grouped, y_test_grouped = train_test_split(\n    X_grouped, y_grouped, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "code/eda-code/eda-ev-tax.html#scatterplot-relationship---predictor-vs.-independent-variable",
    "href": "code/eda-code/eda-ev-tax.html#scatterplot-relationship---predictor-vs.-independent-variable",
    "title": "Log-Transform Predictors",
    "section": "Scatterplot Relationship - Predictor vs. Independent Variable",
    "text": "Scatterplot Relationship - Predictor vs. Independent Variable\n\nimport matplotlib.pyplot as plt\n\n# Scatter plots to understand relationship between predictors and independent variable\nfor i, feature in enumerate(features_to_use):\n    plt.figure(figsize=(10,5))\n    plt.scatter(X_grouped[feature], y_grouped, c=\"lightblue\", edgecolors=\"blue\")\n    plt.title(f'{feature} vs. Vehicle Count Relationship')\n    plt.xlabel(feature)\n    plt.ylabel('Vehicle Count')\n    plt.savefig(f'../../results/figures/scatterplots/{feature}_scatterplot.png')\n    plt.show()"
  },
  {
    "objectID": "code/eda-code/eda-ev-tax.html#histogram-distributions",
    "href": "code/eda-code/eda-ev-tax.html#histogram-distributions",
    "title": "Log-Transform Predictors",
    "section": "Histogram Distributions",
    "text": "Histogram Distributions\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Define a function to log transform all features\ndef log_transform_all_features(data):\n    return np.log(data)\n\n# Impute missing values with the median and then log transform all features\ndef preprocess_and_log_transform_data(X):\n    imputer = SimpleImputer(strategy='median')\n    X_imputed = imputer.fit_transform(X)\n    X_imputed_df = pd.DataFrame(X_imputed, columns=X.columns)\n    X_transformed = log_transform_all_features(X_imputed_df)\n    return X_transformed\n\n# Preprocess and log transform the training and test datasets\nX_transformed = preprocess_and_log_transform_data(X_grouped)\n\n# Plot histogram for each feature in the transformed dataset\ndef plot_transformed_feature_histograms(data, data1):\n\n    for feature in data.columns:\n        fig, ax = plt.subplots(1, 2, figsize=(10,5))\n        ax1 = plt.subplot(2,1,1)\n        ax1 = sns.histplot(data[feature], kde=True, bins=50)\n        ax2 = plt.subplot(2,1,2)\n        ax2 = sns.histplot(data1[feature], kde=True, bins=50)\n        ax1.title.set_text(f'Original vs. Log-Transformed: {feature}')\n        plt.xlabel(feature)\n        plt.ylabel('Frequency')\n        plt.grid(True)\n        plt.tight_layout()\n        plt.savefig(f'../../results/figures/histograms/{feature}_histogram.png')\n        plt.show()\n\n# Plot the histograms for the log-transformed features\nplot_transformed_feature_histograms(X_grouped, X_transformed)"
  },
  {
    "objectID": "code/eda-code/eda-ev-tax.html#boxplot-distributions",
    "href": "code/eda-code/eda-ev-tax.html#boxplot-distributions",
    "title": "Log-Transform Predictors",
    "section": "Boxplot Distributions",
    "text": "Boxplot Distributions\n\n# Plot boxplot for each feature in the transformed dataset\ndef plot_transformed_feature_boxplots(data, data1):\n\n    for feature in data.columns:\n        fig, ax = plt.subplots(1, 2, figsize=(10,5))\n        ax1 = plt.subplot(2,1,1)\n        ax1 = sns.boxplot(data[feature])\n        ax2 = plt.subplot(2,1,2)\n        ax2 = sns.boxplot(data1[feature])\n        ax1.title.set_text(f'Original vs. Log-Transformed Boxplot of {feature}')\n        plt.xlabel(feature)\n        plt.ylabel('Frequency')\n        plt.grid(True)\n        plt.tight_layout()\n        plt.savefig(f'../../results/figures/boxplots/{feature}_boxplot.png')\n        plt.show()\n\n# Plot the boxplots for the log-transformed features\nplot_transformed_feature_boxplots(X_grouped, X_transformed)"
  },
  {
    "objectID": "code/processing-code/pre_process_tax_data.html",
    "href": "code/processing-code/pre_process_tax_data.html",
    "title": "Pre-Process Tax Data",
    "section": "",
    "text": "from pandas import read_csv, set_option, merge\nfrom numpy import mean\nset_option('display.max_columns', None)"
  },
  {
    "objectID": "code/processing-code/pre_process_tax_data.html#import-libraries",
    "href": "code/processing-code/pre_process_tax_data.html#import-libraries",
    "title": "Pre-Process Tax Data",
    "section": "",
    "text": "from pandas import read_csv, set_option, merge\nfrom numpy import mean\nset_option('display.max_columns', None)"
  },
  {
    "objectID": "code/processing-code/pre_process_tax_data.html#reformat-data-dictionary-names",
    "href": "code/processing-code/pre_process_tax_data.html#reformat-data-dictionary-names",
    "title": "Pre-Process Tax Data",
    "section": "Reformat Data Dictionary Names",
    "text": "Reformat Data Dictionary Names\n\ndef data_dict_pre_process(filename: str):\n    strip_variable_whitespace_list = []\n    data_dict_df = read_csv(f'../../data/raw-data/{filename}.csv')\n    for idx, row in data_dict_df.iterrows():\n        variable = row['VARIABLE NAME'].strip()\n        strip_variable_whitespace_list.append(variable)\n    data_dict_df['VARIABLE NAME'] = strip_variable_whitespace_list\n\n    data_dict_df_variable_names = data_dict_df['VARIABLE NAME'].tolist()\n    \n    tax_qualitative_variables = data_dict_df_variable_names[:19]\n    format_tax_qualitative_variables = []\n    for code in tax_qualitative_variables:\n        format_tax_qualitative_variables.append(code)\n    \n    tax_numerical_variables = data_dict_df_variable_names[19:]\n    total_count_return_variables = []\n    total_amount_return_variables = []\n    for code in tax_numerical_variables:\n        if code.startswith('N'):\n            total_count_return_variables.append(code)\n        else:\n            total_amount_return_variables.append(code)\n\n    amount_return_variables_new = format_tax_qualitative_variables + total_amount_return_variables\n    count_return_variables_new = tax_qualitative_variables + total_count_return_variables\n    description_amount_data_dict_list = data_dict_df['DESCRIPTION'].loc[data_dict_df['VARIABLE NAME'].isin(amount_return_variables_new)].tolist()\n    description_count_data_dict_list = data_dict_df['DESCRIPTION'].loc[data_dict_df['VARIABLE NAME'].isin(count_return_variables_new)].tolist()\n\n    amount_data_dict_df = data_dict_df.loc[data_dict_df['VARIABLE NAME'].isin(amount_return_variables_new)]\n    count_data_dict_df = data_dict_df.loc[data_dict_df['VARIABLE NAME'].isin(count_return_variables_new)]\n    amount_data_dict_df.to_csv('../../data/intermediate-data/data_dict_variable_total_amount.csv', index=False)\n    count_data_dict_df.to_csv('../../data/intermediate-data/data_dict_variable_total_number_returns.csv', index=False)"
  },
  {
    "objectID": "code/processing-code/pre_process_tax_data.html#preprocess-tax-data",
    "href": "code/processing-code/pre_process_tax_data.html#preprocess-tax-data",
    "title": "Pre-Process Tax Data",
    "section": "Preprocess Tax Data",
    "text": "Preprocess Tax Data\n\ndef tax_data_pre_process(filename: str, state: str):\n    tax_data_df = read_csv(f'../../data/raw-data/{filename}.csv').drop('agi_stub', axis=1)\n    tax_data_df = tax_data_df.loc[tax_data_df['STATE'] == f'{state}']\n\n    amount_data_dict_df = read_csv(f'../../data/intermediate-data/data_dict_variable_total_amount.csv')\n    amount_data_dict_df_variables_list = amount_data_dict_df['VARIABLE NAME'].tolist()\n    description_amount_list = amount_data_dict_df['DESCRIPTION'].tolist()\n    total_amount_tax_data_df = tax_data_df.loc[:, tax_data_df.columns.isin(amount_data_dict_df_variables_list)]\n    total_amount_tax_data_df.columns = description_amount_list\n    total_amount_tax_data_df = total_amount_tax_data_df.rename(\n        columns={\n            '5-digit Zip code': 'ZIPCODE'\n        }\n    )\n    total_amount_tax_data_df.to_csv(f'../../data/intermediate-data/{state}_total_amount_tax_data.csv', index=False)\n    \n    number_data_dict_df = read_csv(f'../../data/intermediate-data/data_dict_variable_total_number_returns.csv')\n    number_data_dict_df_variables_list = number_data_dict_df['VARIABLE NAME'].tolist()\n    description_number_list = number_data_dict_df['DESCRIPTION'].tolist()\n    total_number_tax_data_df = tax_data_df.loc[:, tax_data_df.columns.isin(number_data_dict_df_variables_list)]\n    total_number_tax_data_df.columns = description_number_list\n    total_number_tax_data_df = total_number_tax_data_df.rename(\n        columns={\n            '5-digit Zip code': 'ZIPCODE'\n        }\n    )\n    total_number_tax_data_df.to_csv(f'../../data/intermediate-data/{state}_total_number_returns_tax_data.csv', index=False)"
  },
  {
    "objectID": "code/processing-code/pre_process_tax_data.html#preprocess-ev-data",
    "href": "code/processing-code/pre_process_tax_data.html#preprocess-ev-data",
    "title": "Pre-Process Tax Data",
    "section": "Preprocess EV Data",
    "text": "Preprocess EV Data\n\ndef ev_data_pre_process(state: str):\n    ev_data_df = read_csv('../../data/raw-data/Electric_Vehicle_Population_Data.csv').rename(columns={\n        'Postal Code': 'ZIPCODE'\n    })\n    ev_data_df['ZIPCODE'] = ev_data_df['ZIPCODE'].fillna(0).astype(int)\n    null_values = ev_data_df.isnull().sum()\n    ev_data_df.dropna(inplace=True)\n    ev_data_df_2020 = ev_data_df.loc[(ev_data_df['Model Year'] &lt;= 2020) & (ev_data_df['Electric Range'] &gt; 0) & (ev_data_df['Model Year'] &gt; 2010) & (ev_data_df['State'] == f'{state}')]\n    ev_duplicated_df = ev_data_df[ev_data_df.duplicated()]\n\n    zip_code_count_brand_model_year = ev_data_df_2020.groupby(['City', 'State', 'ZIPCODE', 'Model Year', 'Make', 'Model', 'Electric Vehicle Type', 'Clean Alternative Fuel Vehicle (CAFV) Eligibility']).agg({\n        'Electric Range': 'mean',\n        'County': 'count',\n    }).reset_index()\n    zip_code_count_brand_model = ev_data_df_2020.groupby(['City', 'State', 'ZIPCODE', 'Make', 'Model', 'Electric Vehicle Type', 'Clean Alternative Fuel Vehicle (CAFV) Eligibility']).agg({\n        'Electric Range': 'mean',\n        'County': 'count',\n    }).reset_index()\n    zip_code_count_brand = ev_data_df_2020.groupby(['City', 'State', 'ZIPCODE', 'Make', 'Electric Vehicle Type', 'Clean Alternative Fuel Vehicle (CAFV) Eligibility']).agg({\n        'Electric Range': 'mean',\n        'County': 'count',\n    }).reset_index()\n    zip_code_count_brand_model_year.to_csv(f'../../data/intermediate-data/{state}_zip_code_brand_model_year.csv', index=False)\n    zip_code_count_brand_model.to_csv(f'../../data/intermediate-data/{state}_zip_code_brand_model.csv', index=False)\n    zip_code_count_brand.to_csv(f'../../data/intermediate-data/{state}_zip_code_brand.csv', index=False)"
  },
  {
    "objectID": "code/processing-code/pre_process_tax_data.html#join-taxev-data",
    "href": "code/processing-code/pre_process_tax_data.html#join-taxev-data",
    "title": "Pre-Process Tax Data",
    "section": "Join Tax/EV Data",
    "text": "Join Tax/EV Data\n\ndef join_tax_ev_data(state: str):\n    zip_code_count_brand_model_year = read_csv(f'../../data/intermediate-data/{state}_zip_code_brand_model_year.csv')\n    zip_code_count_brand_model= read_csv(f'../../data/intermediate-data/{state}_zip_code_brand_model.csv')\n    zip_code_count_brand = read_csv(f'../../data/intermediate-data/{state}_zip_code_brand.csv')\n    amount_tax_data = read_csv(f'../../data/intermediate-data/{state}_total_amount_tax_data.csv')\n\n    final_zip_code_count_brand_model_year = merge(zip_code_count_brand_model_year, amount_tax_data, on='ZIPCODE').rename(\n        columns={\n            'County': 'Vehicle Count'\n        }\n    )\n    final_zip_code_count_brand_model = merge(zip_code_count_brand_model, amount_tax_data, on='ZIPCODE').rename(\n        columns={\n            'County': 'Vehicle Count'\n        }\n    )\n    final_zip_code_count_brand = merge(zip_code_count_brand, amount_tax_data, on='ZIPCODE').rename(\n        columns={\n            'County': 'Vehicle Count'\n        }\n    )\n    final_zip_code_count_brand_model_year = final_zip_code_count_brand_model_year.drop(\n        [\n            'Adjust gross income (AGI) [8]',\n            'Number of volunteer prepared returns with Earned Income Credit [5]',\n            'Total income amount',\n            'Total statutory adjustments amount',\n            'Total standard deduction amount',\n            'Total itemized deductions amount',\n            'Amount of AGI for itemized returns',\n            'Total taxes paid amount',\n            'Taxable income amount',\n            'Income tax before credits amount',\n            'Alternative minimum tax amount',\n            'Total tax credits amount',\n            'Total premium tax credit amount',\n            'Total tax payments amount',\n            'Income tax after credits amount   ',\n            'Total tax liability amount [18]  ',\n            'Tax due at time of filing amount [19]  ',\n            'Total overpayments amount',\n            'Overpayments refunded amount [20]  ',\n        ],\n        axis = 1\n    )\n    final_zip_code_count_brand_model_year.to_csv('../../data/processed-data/brand_model_year_tax_data.csv', index=False)\n    final_zip_code_count_brand_model.to_csv('../../data/processed-data/brand_model_tax_data.csv', index=False)\n    final_zip_code_count_brand.to_csv('../../data/processed-data/brand_tax_data.csv', index=False)"
  },
  {
    "objectID": "code/processing-code/pre_process_tax_data.html#execute-functions-for-final-dataset",
    "href": "code/processing-code/pre_process_tax_data.html#execute-functions-for-final-dataset",
    "title": "Pre-Process Tax Data",
    "section": "Execute Functions For Final Dataset",
    "text": "Execute Functions For Final Dataset\n\ndata_dict_pre_process(filename='data_dict2021')\ntax_data_pre_process(filename='tax_data_2021', state='WA')\nev_data_pre_process(state='WA')\njoin_tax_ev_data(state='WA')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EV Predictive Modeling",
    "section": "",
    "text": "This study investigates the use of advanced machine learning models to predict vehicle count by zip code, utilizing a dataset enriched with socio-economic, demographic, and regional features. The primary aim is to accurately estimate vehicle counts, aiding in efficient resource distribution and urban planning. We implemented and evaluated several models, including Decision Tree, Random Forest, XGBoost, and HistGradientBoosting, focusing on their accuracy, interpretability, and computational efficiency.\nEach model was trained and validated using cross-validation and a hold-out test set, with performance metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared used to assess their effectiveness. Among the models, the Random Forest model demonstrated superior performance, offering the best balance of predictive accuracy and model robustness. It effectively captured the complex interactions between the features and the target variable, outperforming the other models in terms of both accuracy and generalization.\nThe results highlight the XGBoost model’s effectiveness in handling diverse and complex datasets, making it a valuable tool for predicting vehicle counts in different zip codes. This study provides critical insights into the practical applications of machine learning in urban planning and resource management, particularly in the context of transportation and infrastructure development."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "EV Predictive Modeling",
    "section": "",
    "text": "This study investigates the use of advanced machine learning models to predict vehicle count by zip code, utilizing a dataset enriched with socio-economic, demographic, and regional features. The primary aim is to accurately estimate vehicle counts, aiding in efficient resource distribution and urban planning. We implemented and evaluated several models, including Decision Tree, Random Forest, XGBoost, and HistGradientBoosting, focusing on their accuracy, interpretability, and computational efficiency.\nEach model was trained and validated using cross-validation and a hold-out test set, with performance metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared used to assess their effectiveness. Among the models, the Random Forest model demonstrated superior performance, offering the best balance of predictive accuracy and model robustness. It effectively captured the complex interactions between the features and the target variable, outperforming the other models in terms of both accuracy and generalization.\nThe results highlight the XGBoost model’s effectiveness in handling diverse and complex datasets, making it a valuable tool for predicting vehicle counts in different zip codes. This study provides critical insights into the practical applications of machine learning in urban planning and resource management, particularly in the context of transportation and infrastructure development."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "EV Predictive Modeling",
    "section": "Introduction",
    "text": "Introduction\nThe increasing adoption of electric vehicles (EVs) is a pivotal shift in the transportation sector, driven by growing environmental concerns, advancements in technology, and supportive government policies. Understanding the factors that influence the distribution of EVs is essential for developing targeted policies and infrastructure, such as charging stations and incentives. However, EV adoption is not uniform across regions; it varies significantly based on socio-economic and demographic factors. This study aims to explore these variations by predicting vehicle counts, particularly EVs, at the zip code level using a comprehensive dataset that combines electric vehicle registration data  (n.d.-a) with individual income tax statistics from the Internal Revenue Service (IRS)  (n.d.-b).\nThe IRS data offers detailed insights into various financial attributes of taxpayers by zip code, including income levels, deductions, and credits. By integrating this data with EV registration data, we can analyze the relationship between financial capacity, tax-related benefits, and vehicle ownership patterns. Such an analysis is crucial for identifying the socio-economic determinants of EV adoption, which can inform policymakers and urban planners.\nThe main objectives of this study are twofold: first, to develop predictive models that accurately estimate vehicle counts by zip code, and second, to identify and quantify the impact of key socio-economic factors on these counts. We employed a variety of machine learning models, including Decision Tree, Random Forest, XGBoost, and HistGradientBoosting, to achieve these objectives. These models were chosen for their ability to handle complex, structured data and provide interpretable results.\nThe findings of this study are expected to contribute valuable insights into the socio-economic factors driving EV adoption. By identifying the key predictors of vehicle counts, we can better understand the barriers to and facilitators of EV adoption in different communities. This knowledge is crucial for designing effective policies and interventions that promote sustainable transportation solutions. Furthermore, the study’s methodology, which includes rigorous data cleaning, feature transformation, and model validation, provides a robust framework for future research in this area."
  },
  {
    "objectID": "index.html#methods",
    "href": "index.html#methods",
    "title": "EV Predictive Modeling",
    "section": "Methods",
    "text": "Methods\nIn this study, we employed a variety of machine learning models to predict vehicle counts by zip code using socio-economic and demographic features extracted from IRS tax data. The data preparation, feature transformation, and model training processes are detailed below.\n\nData Preparation & Exploratory Analysis\nThe dataset was compiled by merging electric vehicle registration data with IRS tax data, focusing specifically on zip codes within the state of Washington. This geographical focus provided a diverse socio-economic landscape, enhancing the robustness of our analysis. The dataset included features such as ‘Net premium tax credit amount,’ ‘Home mortgage interest paid amount,’ ‘Real estate taxes amount,’ and several others related to income, tax credits, and deductions.\nTo prepare the dataset for analysis, we first addressed data quality issues. We standardized column names using regular expressions, ensuring consistency across the merged dataset. This step was essential for preventing errors during data processing and analysis. We also applied data filtering to include only the relevant zip codes from Washington state, further refining our dataset for localized analysis.\n\nScatterplots - Predictor’s vs. Vehicle Count Relationship\n\n\n\nSE Health Insurance Deduction Scatterplot\n\n\n\n\n\nFeature Transformation\nGiven that many of the predictor variables exhibited skewed distributions, we applied a log transformation to normalize these features. This transformation was particularly necessary for financial features, such as ‘Salaries and wages amount’ and ‘Home mortgage interest paid amount,’ which spanned several orders of magnitude. The transformation formula used was log⁡(x+1)log(x+1), which effectively handled zero values and stabilized variance across the dataset. Normalizing the data in this manner helped improve the performance and interpretability of our machine learning models by reducing the impact of outliers and facilitating a more uniform distribution of feature values.\n\nLog-Transform Histogram\n\n\n\nSE Health Insurance Deduction\n\n\n\n\nLog-Transform Boxplot\n\n\n\nSE Health Insurance Deduction Boxplot\n\n\n\n\n\nModel Development\nWe implemented four different machine learning models: Decision Tree, Random Forest, XGBoost, and HistGradientBoosting. Each model was selected for its unique strengths in handling structured data and complex relationships between features.\n\n\nModel Training and Validation\nEach model was trained on the processed dataset, with training and validation sets created using cross-validation techniques. Cross-validation helped ensure that the models generalize well to unseen data, preventing overfitting. We evaluated model performance using metrics such as Root Mean Squared Error (RMSE) and R-squared, which provided insights into the accuracy and explanatory power of each model.\nOverall, the methods employed in this study—from data preparation and feature transformation to model development and validation—were designed to rigorously test and compare the predictive capabilities of multiple machine learning models. These methods provided a comprehensive framework for analyzing the socio-economic factors influencing vehicle counts by zip code, with a particular emphasis on the adoption of electric vehicles."
  },
  {
    "objectID": "index.html#results",
    "href": "index.html#results",
    "title": "EV Predictive Modeling",
    "section": "Results",
    "text": "Results\nThis section presents the comparative analysis of various machine learning models used to predict vehicle count by zip code. The models evaluated include XGBoost, Decision Tree, HistGradientBoosting, and Random Forest. The primary evaluation metrics considered were Root Mean Squared Error (RMSE) and R-squared, which provide insights into the accuracy and explanatory power of the models.\n\nModel Performance Comparison\n\nXGBoost\n\nRMSE: 69.21 \nR-squared: 0.892 \nXGBoost demonstrated strong predictive capabilities, achieving a relatively low RMSE of 71.27. The model’s R-squared value of 0.91 indicates that it explains 89.2% of the variance in the vehicle count data, suggesting good model fit and predictive accuracy.  \n\n\n\nDecision Tree\n\nRMSE: 79.61\nR-squared: 0.857\nThe Decision Tree model had the highest RMSE among the evaluated models at 79.61, reflecting less precise predictions. Its R-squared value of 0.857 indicates that it explains 85.7% of the variance, making it the least accurate model in this comparison. \n\n\n\nHistGradientBoosting\n\nRMSE: 76.96\nR-squared: 0.867\nThe HistGradientBoosting model offered a moderate performance with an RMSE of 76.96 and an R-squared value of 0.867. While better than the Decision Tree model, it still lagged behind XGBoost and Random Forest in terms of prediction accuracy and variance explanation. \n\n\n\nRandom Forest\n\nRMSE: 71.30\nR-squared: 0.885\nThe Random Forest model outperformed all other models, achieving the lowest RMSE of 71.30 and the highest R-squared value of 0.885. These metrics indicate that the Random Forest model not only provided the most accurate predictions but also explained 88.5% of the variance in vehicle counts, making it the most reliable model in this analysis. \n\n\n\n\nBest Model Selection\nThe XGBoost model emerged as the best performer in predicting vehicle counts by zip code, primarily due to its advanced boosting techniques and the ability to handle various data complexities effectively. The combination of these features makes XGBoost a powerful and versatile tool for predictive modeling. Its ability to handle complex, structured data, prevent overfitting, and efficiently process large datasets, while offering detailed feature importance analysis, made it the top performer in this study. The model’s superior performance in terms of low RMSE and high R-squared values underscores its effectiveness in accurately predicting vehicle counts by zip code, providing valuable insights for urban planning and policy-making."
  },
  {
    "objectID": "index.html#discussion",
    "href": "index.html#discussion",
    "title": "EV Predictive Modeling",
    "section": "Discussion",
    "text": "Discussion\nThe top 10 features identified as the most significant predictors of vehicle counts by zip code offer insights into the socio-economic and financial characteristics of the population. These features provide a nuanced understanding of the factors that potentially influence vehicle ownership, particularly the adoption of electric vehicles (EVs). Here’s a detailed discussion of each feature and its relevance:"
  },
  {
    "objectID": "index.html#variable-importance---xgboost",
    "href": "index.html#variable-importance---xgboost",
    "title": "EV Predictive Modeling",
    "section": "Variable Importance - XGBoost",
    "text": "Variable Importance - XGBoost\n\n\n\n‘XGBoost’\n\n\n\nSelf-Employed Health Insurance Deduction Amount: This feature represents the amount self-employed individuals can deduct for health insurance premiums. It is significant as it indicates the presence and financial activities of self-employed individuals within a zip code. Self-employed individuals often have variable incomes and unique financial situations, which may influence their ability to purchase and maintain vehicles. This deduction could correlate with areas where self-employment is prevalent, possibly reflecting regions with more entrepreneurial activity and, potentially, higher disposable incomes.\nIndividual Retirement Arrangement Payments Amount Contributions to Individual Retirement Arrangements (IRAs) reflect financial planning and savings behavior. Higher contributions suggest higher disposable income and financial stability, factors that are positively associated with the ability to purchase vehicles, including more expensive options like EVs. This feature indicates regions where residents prioritize long-term financial health, which may correlate with the capacity to invest in newer, environmentally friendly technologies.\nHome Mortgage from Personal Seller Amount: This feature involves transactions where the home mortgage is obtained directly from a personal seller rather than a traditional financial institution. It indicates non-standard financial arrangements, possibly reflecting unique housing markets or economic conditions in certain areas. The relevance of this feature could be tied to the accessibility and flexibility in financial transactions within these regions, potentially influencing overall economic activity, including vehicle purchases.\nSalaries and Wages Amount: A direct indicator of income, this feature represents the total salaries and wages reported within a zip code. It is a critical predictor as higher salaries and wages generally correlate with greater financial capacity to purchase and maintain vehicles. This feature strongly influences vehicle ownership levels, including the adoption of EVs, which may have higher upfront costs but offer long-term savings.\nHome Mortgage Interest Paid Amount: The amount of interest paid on home mortgages is indicative of both property ownership and the financial burden of maintaining such properties. Higher mortgage interest payments suggest higher home values and, by extension, potentially higher wealth levels. This wealth is a significant factor in vehicle ownership, as individuals with more expensive homes often have higher disposable incomes, enabling them to invest in additional vehicles or more luxurious options, including EVs.\nSelf-Employment Tax Amount: This feature captures the amount of self-employment tax paid, which directly correlates with self-employment income. Higher self-employment tax payments indicate successful businesses or high-earning self-employed individuals. The economic health of these individuals likely contributes to their ability to afford vehicles, making this a significant predictor of vehicle counts.\nSelf-Employed (Keogh) Retirement Plans Amount Contributions to Keogh plans reflect savings set aside by self-employed individuals for retirement. Like IRA contributions, this feature suggests a focus on long-term financial security, indicative of higher income levels and financial planning. This financial stability is associated with greater capacity for discretionary spending, including vehicle purchases.\nBusiness or Professional Net Income (Less Loss) Amount: This feature represents the net income from business or professional activities, after accounting for losses. It is a measure of the economic success and profitability of businesses within a zip code. Higher net incomes suggest prosperous business environments, which can lead to increased vehicle ownership among business owners and employees, reflecting economic prosperity in these areas.\nReal Estate Taxes Amount: Real estate taxes are directly linked to property values and indirectly to wealth. Higher real estate taxes generally indicate more expensive properties, suggesting higher overall wealth in an area. Wealthier populations have greater access to credit and financing options, which can increase the likelihood of purchasing vehicles, including newer and more expensive models like EVs.\nNet Premium Tax Credit Amount: The net premium tax credit amount provides a tax credit under the Affordable Care Act, generally aimed at reducing the cost of health insurance for lower to middle-income families. While it may seem counterintuitive, this feature could correlate with regions where financial assistance is necessary, reflecting economic diversity. In these areas, the adoption of vehicles, including EVs, may be influenced by financial incentives or subsidies aimed at promoting energy-efficient technologies."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "EV Predictive Modeling",
    "section": "Conclusion",
    "text": "Conclusion\nThe Random Forest model’s ability to incorporate and weigh these diverse socio-economic features underscores its robustness and suitability for this type of predictive analysis. The insights gained from the top features not only enhance our understanding of the factors driving vehicle ownership but also offer valuable guidance for policymakers and urban planners aiming to promote EV adoption. Future research could expand on these findings by incorporating additional data sources, such as environmental factors or public transportation availability, to provide a more comprehensive view of the determinants of vehicle ownership and EV adoption."
  }
]